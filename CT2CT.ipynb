{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT2CT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPXe2faQCy2c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **CT2CT: Resolution Augmentation of Computed Tomography (CT) Scans** \n",
        "*by Sergio Rivera*\n",
        "Please, follow the execution of the cells listed below to understand and use the model.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq7x8TsdZsAz",
        "colab_type": "text"
      },
      "source": [
        "# **1. LOAD THE MODEL**\n",
        "In this first step data from a default scan will be fetched, generator and discriminator will be defined and compiled, I/O images will be generated and all the required parameters will be set ready for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NODNv9asVOsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageChops\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# GPU Check - only for Colab env\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at {}'.format(device_name))\n",
        "\n",
        "# CONNECT TO DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "PATH = \"/content/drive/My Drive/brain_ct\"\n",
        "ATTEMPT_PATH = PATH + \"/attempts\"\n",
        "SAVED_MODELS_PATH = PATH + \"/savedModels\"\n",
        "\n",
        "# DELETE PREVIOUS CONTENT - only delete attempts folder in the future\n",
        "shutil.rmtree(ATTEMPT_PATH)\n",
        "os.makedirs(ATTEMPT_PATH)\n",
        "\n",
        "# GLOBAL VARIABLES\n",
        "RANDOM_SEED = 1234\n",
        "IMG_WIDTH = 512\n",
        "IMG_HEIGHT = 512\n",
        "\n",
        "# FETCH DATA\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "import natsort\n",
        "\n",
        "print(\"Downloading scan...\")\n",
        "resp = urlopen(\"https://raw.github.com/sergiorivera50/CT2CT/master/rawIMG.zip\")\n",
        "zip_file = ZipFile(BytesIO(resp.read()))\n",
        "file_names = zip_file.namelist()\n",
        "file_names = natsort.natsorted(file_names,reverse=False) # Sort filenames (1 -> 240)\n",
        "RAW_IMAGES = []\n",
        "\n",
        "for i in range(len(file_names)):\n",
        "  content = zip_file.read(file_names[i]) # Reads content\n",
        "  img = Image.open(BytesIO(content)) # Converts to image\n",
        "  RAW_IMAGES.append(img) # Stores image into RAW_IMAGES array\n",
        "\n",
        "slices = len(RAW_IMAGES)\n",
        "\n",
        "# GENERATE INPUT/OUTPUT\n",
        "import cv2\n",
        "\n",
        "def rgbImg2GreyImg(rgb_img):\n",
        "  return cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "print(\"Generating I/O...\")\n",
        "\n",
        "INPUT_IMAGES = np.empty((slices-2, IMG_HEIGHT, IMG_WIDTH, 1), dtype=\"float32\")\n",
        "OUTPUT_IMAGES = np.empty((slices-2, IMG_HEIGHT, IMG_WIDTH, 1), dtype=\"float32\")\n",
        "\n",
        "i = 0\n",
        "while i < (slices-2):\n",
        "    A = RAW_IMAGES[i] # Image 1\n",
        "    C = RAW_IMAGES[i+2] # Image 3\n",
        "    B = RAW_IMAGES[i+1] # Image 2\n",
        "    \n",
        "    # Save middle image (B)(OUT)\n",
        "    OUTPUT_IMAGES[i] = np.resize(rgbImg2GreyImg(np.float32(B)), (IMG_HEIGHT, IMG_WIDTH, 1))\n",
        "    \n",
        "    # Combine and save AmC\n",
        "    combined = cv2.addWeighted(np.float32(A), 0.5, np.float32(C), 0.5, 0)\n",
        "    INPUT_IMAGES[i] = np.resize(rgbImg2GreyImg(combined), (IMG_HEIGHT, IMG_WIDTH, 1))\n",
        "    i += 1\n",
        "\n",
        "# Normalize images\n",
        "def normalize(inimg, tgimg):\n",
        "    inimg = (inimg / 128) - 1\n",
        "    tgimg = (tgimg / 128) - 1\n",
        "\n",
        "    return inimg, tgimg\n",
        "\n",
        "print(\"Preprocessing images...\")\n",
        "for i in range(slices-2):\n",
        "  INPUT_IMAGES[i], OUTPUT_IMAGES[i] = normalize(INPUT_IMAGES[i], OUTPUT_IMAGES[i])\n",
        "\n",
        "print(\"Shuffling and splitting train/test...\")\n",
        "# CODE CITED FROM: https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        "# END CITE\n",
        "\n",
        "# TRAIN/TEST\n",
        "\n",
        "train_n = round(slices * 0.80) # Train/Test 0.8/0.2\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "RAND_INPUT_IMAGES, RAND_OUTPUT_IMAGES = shuffle_in_unison(INPUT_IMAGES, OUTPUT_IMAGES)\n",
        "\n",
        "tr_input_imgs = RAND_INPUT_IMAGES[:train_n]\n",
        "tr_output_imgs = RAND_OUTPUT_IMAGES[:train_n]\n",
        "\n",
        "ts_input_imgs = RAND_INPUT_IMAGES[train_n:slices]\n",
        "ts_output_imgs = RAND_OUTPUT_IMAGES[train_n:slices]\n",
        "\n",
        "print(\"slices= {} | train= {} | test= {}\".format(slices, len(tr_input_imgs), len(ts_input_imgs)))\n",
        "\n",
        "# CREATE DATASETS\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((tr_input_imgs, tr_output_imgs))\n",
        "train_dataset = train_dataset.batch(1)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((ts_input_imgs, ts_output_imgs))\n",
        "test_dataset = test_dataset.batch(1)\n",
        "\n",
        "# ENCODER\n",
        "# C64-C128-C256-C512-C512-C512-C512-C512\n",
        "def downsample(filters, apply_batchnorm=True):\n",
        "  \n",
        "    result = Sequential()\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02) # mean 0 deviation 0.02\n",
        "\n",
        "    # Convolutional layer\n",
        "    result.add(Conv2D(filters,\n",
        "                    kernel_size=4,\n",
        "                    strides=2, # Each layer info shrinks by 1 / strides\n",
        "                    padding=\"same\",\n",
        "                    kernel_initializer=initializer,\n",
        "                    use_bias=not apply_batchnorm)) # Only apply if not using BatchNorm\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        # BatchNorm layer\n",
        "        result.add(BatchNormalization())\n",
        "\n",
        "    # Activation layer\n",
        "    result.add(LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "# DECODER\n",
        "# CD512-CD512-CD512-C512-C256-C128-C64\n",
        "def upsample(filters, apply_dropout=False):\n",
        "  \n",
        "    result = Sequential()\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02) # mean 0 deviation 0.02\n",
        "\n",
        "    # Convolutional layer\n",
        "    result.add(Conv2DTranspose(filters, #Conv2D inverse\n",
        "                             kernel_size=4,\n",
        "                             strides=2,\n",
        "                             padding=\"same\",\n",
        "                             kernel_initializer=initializer,\n",
        "                             use_bias=False))\n",
        "\n",
        "    # BatchNorm layer\n",
        "    result.add(BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        # Dropout layer (disconnects random connections, regularized)\n",
        "        result.add(Dropout(0.5))\n",
        "\n",
        "    # Activation layer\n",
        "    result.add(ReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "# GENERATOR\n",
        "def Generator():\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=[None, None, 1]) # Not specified dimensions, 1 channel\n",
        "\n",
        "    # C64-C128-C256-C512-C512-C512-C512-C512\n",
        "    down_stack = [                        # (batch_size, width, height, filters)\n",
        "      downsample(64, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "      downsample(128),                       # (bs, 64, 64, 128)\n",
        "      downsample(256),                       # (bs, 32, 32, 256)\n",
        "      downsample(512),                       # (bs, 16, 16, 512)\n",
        "      downsample(512),                       # (bs, 8, 8, 512)\n",
        "      downsample(512),                       # (bs, 4, 4, 512)\n",
        "      downsample(512),                       # (bs, 2, 2, 512)\n",
        "      downsample(512),                       # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    # CD512-CD512-CD512-C512-C256-C128-C64\n",
        "    up_stack = [\n",
        "      upsample(512, apply_dropout=True),     # (bs, 2, 2, 1024)\n",
        "      upsample(512, apply_dropout=True),     # (bs, 4, 4, 1024)\n",
        "      upsample(512, apply_dropout=True),     # (bs, 8, 8, 1024)\n",
        "      upsample(512),                         # (bs, 16, 16, 1024)\n",
        "      upsample(256),                         # (bs, 32, 32, 512)\n",
        "      upsample(128),                         # (bs, 64, 64, 256)\n",
        "      upsample(64),                          # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02)\n",
        "\n",
        "    # Last layer = generated image\n",
        "    last = Conv2DTranspose(filters=1, # filters = 1 made discriminator layers to be [None, None, None, 2], fixing all 1 channel bugs\n",
        "                         kernel_size=4,\n",
        "                         strides=2,\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer=initializer,\n",
        "                         activation=\"tanh\" # de -1 a 1\n",
        "                         )\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    s = [] # Skip Connections list\n",
        "\n",
        "    concat = Concatenate()\n",
        "\n",
        "    # Connect layers\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        s.append(x)\n",
        "\n",
        "    s = reversed(s[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, s):\n",
        "\n",
        "        x = up(x)\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    last = last(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=last) # Builds model\n",
        "\n",
        "generator = Generator()\n",
        "\n",
        "# DISCRIMINATOR 70x70\n",
        "# C64-C128-C256-C512\n",
        "def Discriminator():\n",
        "  \n",
        "    # (dim1, dim2, channels) by default from Keras\n",
        "    ini = Input(shape=[None, None, 1], name=\"input_img\")\n",
        "    gen = Input(shape=[None, None, 1], name=\"gener_img\")\n",
        "    \n",
        "    con = concatenate([ini, gen])\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.2)\n",
        "\n",
        "    down1 = downsample(64, apply_batchnorm=False)(con)\n",
        "    down2 = downsample(128)(down1)\n",
        "    down3 = downsample(256)(down2)\n",
        "    down4 = downsample(512)(down3)\n",
        "  \n",
        "    last = tf.keras.layers.Conv2D(filters=1, # Pixel by pixel\n",
        "                                kernel_size=4,\n",
        "                                strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                padding=\"same\")(down4)\n",
        "\n",
        "    return tf.keras.Model(inputs=[ini, gen], outputs=last)\n",
        "\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# OPTIMIZERS\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "# LOSS CALCULATIONS\n",
        "current_gen_loss = 1\n",
        "current_discr_loss = 1\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) # Apply logit function\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n",
        "\n",
        "LAMBDA = 100\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  \n",
        "    #gan_loss = adversary error\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    #mean absolute error, between generated and target\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss) #mas peso a l1_loss que al adversario\n",
        "\n",
        "    return total_gen_loss\n",
        "\n",
        "# IMAGE GENERATION\n",
        "def generate_images_test(model, test_input, tar, save_filename=False):\n",
        "  \n",
        "    prediction = model(test_input, training=False)\n",
        "\n",
        "    if save_filename:\n",
        "        tf.keras.preprocessing.image.save_img(ATTEMPT_PATH + \"/\" + save_filename + \".jpg\", prediction[0,...])\n",
        "\n",
        "# TRAIN STEP\n",
        "@tf.function()\n",
        "def train_step(input_image, target):\n",
        "  # Gradients & Backpropagation\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:\n",
        "\n",
        "      output_image = generator(input_image, training=True)\n",
        "\n",
        "      output_gen_discr = discriminator([output_image, input_image], training=True)\n",
        "\n",
        "      output_trg_discr = discriminator([target, input_image], training=True)\n",
        "\n",
        "      discr_loss = discriminator_loss(output_trg_discr, output_gen_discr)\n",
        "      current_discr_loss = discr_loss\n",
        "\n",
        "      gen_loss = generator_loss(output_gen_discr, output_image, target)\n",
        "      current_gen_loss = gen_loss\n",
        "\n",
        "      # Apply gradients\n",
        "\n",
        "      generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "\n",
        "      discriminator_grads = discr_tape.gradient(discr_loss, discriminator.trainable_variables)\n",
        "\n",
        "      generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
        "\n",
        "      discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\n",
        "\n",
        "# TRAIN FUNCTION\n",
        "def train(dataset, epochs):\n",
        "    print(\"Initiating training...\")\n",
        "    generator_loss_array = []\n",
        "    discriminator_loss_array = [] \n",
        "    epoch = 0\n",
        "    for epoch in range(epochs):\n",
        "        imgi = 0\n",
        "        for input_image, target in dataset:\n",
        "          imgi += 1\n",
        "          train_step(input_image, target)\n",
        "        for input_image, target in dataset.take(1):\n",
        "          output_image = generator(input_image, training=True)\n",
        "          output_gen_discr = discriminator([output_image, input_image], training=True)\n",
        "          output_trg_discr = discriminator([target, input_image], training=True)\n",
        "          discr_loss = discriminator_loss(output_trg_discr, output_gen_discr)\n",
        "          gen_loss = generator_loss(output_gen_discr, output_image, target)\n",
        "          generator_loss_array.append(tf.keras.backend.get_value(gen_loss))\n",
        "          discriminator_loss_array.append(tf.keras.backend.get_value(discr_loss))\n",
        "        print(\"epoch \" + str(epoch+1) + \" - generator loss: \" + str(tf.keras.backend.get_value(gen_loss)) + \" | discriminator loss: \" \n",
        "              + str(tf.keras.backend.get_value(discr_loss)))\n",
        "        imgi = 0\n",
        "        for inp, tar in test_dataset.take(5):\n",
        "            generate_images_test(generator, inp, tar, str(imgi) + \"_e\" + str(epoch))\n",
        "            imgi += 1\n",
        "        epoch += 1\n",
        "    print(\"Model successfully trained!\")\n",
        "    title = [\"Generator Loss\", \"Discriminator Loss\"]\n",
        "    display_list = [generator_loss_array, discriminator_loss_array]\n",
        "    for i in range(2):\n",
        "      plt.subplot(2, 1, i+1)\n",
        "      plt.title(title[i])\n",
        "      plt.plot(range(epochs), display_list[i])\n",
        "      plt.show()\n",
        "print(\"Model successfully loaded!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMX_jLvW92DM",
        "colab_type": "text"
      },
      "source": [
        "# **2. UNDERSTAND THE PROCESS**\n",
        "This cell is here only to explain better the core concept of this model. In order to train the model to predict inner slices, we need to give the algorithm real data examples of this task. Images A and C are merged together into a single image named AmC which is fed to the model, the objective is to generate a new image which resembles an image B (or the inner slice). Take a look at some real examples in this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-J_9k80-knf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 223\n",
        "plt.figure(figsize=(20,20))\n",
        "display_list = [RAW_IMAGES[index], RAW_IMAGES[index+1], RAW_IMAGES[index+2], INPUT_IMAGES[index], OUTPUT_IMAGES[index]]\n",
        "title = [\"Raw Image A\", \"Raw Image B\", \"Raw Image C\", \"Input Image (AmC)\", \"Target Image (B)\"]\n",
        "\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(np.squeeze(display_list[i]), cmap=\"Greys_r\")\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD31Pl8GZ6lN",
        "colab_type": "text"
      },
      "source": [
        "# **3. TRAIN THE MODEL**\n",
        "During this step, the train function will be called and training will begin. The number of epochs will define how many times the model is trained and how long will training take. It has been tested that 10-50 epochs generate good results, although the best generated images were produced in a 175 epoch training, increasing that number will result in a case of overfitting as can be seen below.\n",
        "![Results image](https://raw.github.com/sergiorivera50/CT2CT/master/assets/results.PNG)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwmPeY_3Aba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Best generation result -> 175 epochs\n",
        "epochs = 10\n",
        "train(train_dataset, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhLP0iPjaABl",
        "colab_type": "text"
      },
      "source": [
        "# **4. VIEW THE RESULTS**\n",
        "Take a look at some generated images using the model trained in the previous step. If the model you are using has just been trained and compiled in cells 1 and 3, these 5 images will also be stored in the *brain_ct/attempts* folder with a prediction made on each epoch so that you can appreciate how the generator improves its predictions over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMwShy9TQ-DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "for inp, tar in test_dataset.take(5):\n",
        "  i += 1\n",
        "  prediction = generator(inp, training=False)\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.title(\"Image \" + str(i) + \". Prediction from current generator at \" + str(epochs) + \" epochs.\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(np.squeeze(prediction[0]), cmap=\"Greys_r\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuZbU965aHL5",
        "colab_type": "text"
      },
      "source": [
        "# **4. SAVE THE TRAINED MODEL**\n",
        "This step will save the CT2CT trained model into an h5 formatted file. It will be stored in the folder *brain_ct/savedModels*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "750ISPw_Q5hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = \"ct2ct_trained_\" + str(epochs) + \"e.h5\"\n",
        "generator.save(SAVED_MODELS_PATH + \"/\" + MODEL_NAME)\n",
        "print(\"Saved generator at /savedModels/\" + MODEL_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNU3EkIdaMpN",
        "colab_type": "text"
      },
      "source": [
        "# **5. LOAD A TRAINED MODEL**\n",
        "Executing this cell will load an already trained model. Then you can come back and execute again cell 4 to see the new results of this generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR4miQICQ8Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "MODEL_NAME = \"ct2ct_trained_50e.h5\" # Change this to your trained model\n",
        "generator = load_model(SAVED_MODELS_PATH + \"/\" + MODEL_NAME)\n",
        "print(\"Loaded generator from /savedModels/\" + MODEL_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}